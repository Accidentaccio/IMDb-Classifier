{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo le librerie necessarie per l'addestramento del classificatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from pandas import read_csv\n",
    "from os import chdir\n",
    "from sys import path\n",
    "from gensim.corpora.textcorpus import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_punctuation, strip_numeric, strip_short, stem_text\n",
    "from re import sub, findall\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguiamo il download del dizionario di stopwors dal modulo *nltk* e creo un set come variabile globale che contiene le parole del dizionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download('stopwords')\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dichiariamo la funzione *text_preprocessing* e importiamo al suo interno la variabile globale stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \n",
    "    global stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al suo interno, svolgiamo alcune operazioni per preparare il testo prima di essere processato poi dal classificatore.\n",
    "In particolare qui abbiamo eseguito alcune operazioni relative alle emoji e alla punteggiatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    text = sub(r'(:\\)|:-\\)|:D|:-D)', 'happy', text)\n",
    "    text = sub(r'(:\\(|:-\\()', 'happy', text)\n",
    "    text = sub(r'o.O', 'incredulous', text)\n",
    "    text = sub(r'(!!)+', 'bigExlamation', text)\n",
    "    text = sub(r'!', 'Exlamation', text)\n",
    "    text = sub(r'\\?!\\?', 'Doubtful', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successivamente andiamo a sostituire tutte le parole MAIUSCOLE, con la parola stessa + \"shout\"parola. Questa operazione migliora ulteriormente la comprensione del codice, dal momento che, da un po' di tempo a questa parte, vi è la concezione che scrivere in maiuscolo equivale a gridare. E questa cosa la facciamo notare al classificatore.\n",
    "Successivamente rendiamo tutto il testo minuscolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for item in findall('[A-Z]+[A-Z]+', text):\n",
    "        text = sub(item, f'{item.lower()} shout{item.lower()}', text, count=1)\n",
    "    text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un generatore *filtered_word* che restituirà separatamente tutte le parole del testo che non sono stopwords.\n",
    "Dopodichè le riuniamo nuovamente nella variabile text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    filtered_words = (word for word in text.split() if word not in stops)\n",
    "    text = ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando alcune funzioni presenti in *gensim* andiamo ad effettuare alcune operazioni come la rimozione della punteggiatura, la rimozione dei numeri, la rimozione delle parole più corte di 3 caratteri, la rimozione degli spazi bianchi multipli e infine la funzione restituisce il testo \"stemmatizzato\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    text = strip_punctuation(text)\n",
    "    text = strip_numeric(text)\n",
    "    text = strip_short(text, minsize=3)\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    return stem_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dichiariamo la funzione *main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al suo interno importiamo la variabile globale *stops* e l'istruzione per poter spostarci nella cartella di lavoro in cui è presente il file che viene eseguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    global stops\n",
    "    chdir(path[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inizializziamo un dataframe leggendolo dal file .csv di addestramento, che contiene le colonne *'Review'* e *'Sentiment'*. A tutta la colonna relativa alle recensioni andiamo ad applicare la funzione di preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = read_csv('IMDB Dataset.csv')\n",
    "    df['Review'] = df['Review'].map(text_preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiamo il dataset in variabili indipendenti (x, le recensioni) e dipendenti (y, il risultato associato alla recensione, ovvvero **negativo** o **positivo**). Il parametro *test_size* sta ad indicare che la dimensione del dataset di test sarà 1/3 del totale, e di conseguenza la partizione relativa al dataset di addestramento 2/3 del totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Sentiment'], test_size=0.33, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo un oggetto di tipo **CountVectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su cui viene invocato il metodo *fit_transform* per la creazione della **matrice document-term**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train_matrix = count_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo quindi un oggetto di tipo **TfidfTransformer** su cui verrà invocato il metodo *fit_transform* che restituirà un array con i vari valori TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo il classificatore e lo addestriamo con il dataset di test delle recensioni e dei sentimenti (**X** e **y**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addestrato il classificatore sui 2/3 del Dataset, proviamo a eseguire il test con la restante parte del totale. Procediamo quindi alla stessa maniera di prima, utilizzando però le variabili relative al test e non all'addestramento e il metodo *transform* invece di *fit_transform*.\n",
    "È importante utilizzare lo stesso oggetto **CountVectorizer** e **TfidfTransformer** utilizzati per l'addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_test_matrix = count_vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf_transformer.transform(X_test_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infine creo un array con i risultati del test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predicted = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo poi eventualmente stampare la matrice di confusione e le statistiche relative al classificatore per valutarne l'efficienza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(confusion_matrix(y_test,predicted))\n",
    "    print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inoltre, per poter utilizzare il classificatore in un altro script di Python o a distanza di tempo, per non eseguire di nuovo tutto l'addestramento partendo dal Dataset iniziale, esportiamo i tre oggetti necessari (**Classificatore**, **TfidfTransformer** e **CountVectorizer**) in una cartella del progetto creata appositamente. Questo è possibile grazie al modulo *joblib*. Si può fare altettanto con il modulo *pickle*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dump(clf, './Objects/clf.pkl') \n",
    "    dump(count_vect, './Objects/count_vect.pkl')\n",
    "    dump(tfidf_transformer, './Objects/tfidf_transformer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
